"""ì „ì²´ ìë™í™”: ìƒì„± â†’ ë°œí–‰ (í ê¸°ë°˜)"""

import asyncio
from datetime import datetime
from typing import Optional

from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse
from fastapi.concurrency import run_in_threadpool
from pydantic import BaseModel

from llm.gemini_new_service import gemini_new_gen
from routers.auth.naver import naver_login_with_playwright
from routers.generate.batch import generate_images_parallel, save_to_pending
from utils.get_category_db_name import get_category_db_name
from utils.logger import log

from .common import (
    create_queue,
    get_queue_manuscripts,
    get_base_time,
    calculate_schedule_time,
    update_queue_status,
    cleanup_empty_queue,
    publish_queue_manuscript,
)

router = APIRouter()


class AutoBotRequest(BaseModel):
    account: dict
    keywords: list[str]
    service: str = "default"
    ref: str = ""
    generate_images: bool = True
    image_count: int = 5
    use_schedule: bool = True
    schedule_date: Optional[str] = None
    schedule_start_hour: int = 10
    schedule_interval_hours: int = 1
    delay_between_posts: int = 10


@router.post("/auto")
async def auto_bot(request: AutoBotRequest):
    """ì „ì²´ ìë™í™”: ì›ê³ +ì´ë¯¸ì§€ ìƒì„± â†’ ë¡œê·¸ì¸ â†’ ë°œí–‰"""
    start_ts = datetime.now()
    account_id = request.account.get("id")
    password = request.account.get("password")

    if not account_id or not password:
        raise HTTPException(status_code=400, detail="ê³„ì • ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    log.header("ì „ì²´ ìë™í™” ì‹œì‘", "ğŸ¤–")
    log.kv("ê³„ì •", f"{account_id[:3]}***")
    log.kv("í‚¤ì›Œë“œ", f"{len(request.keywords)}ê°œ")

    # ========== 1ë‹¨ê³„: ì›ê³  ìƒì„± ==========
    log.header("1ë‹¨ê³„: ì›ê³  ìƒì„±", "ğŸ“")
    generated_ids = []

    for idx, keyword in enumerate(request.keywords):
        keyword = keyword.strip()
        if not keyword:
            continue

        log.step(idx + 1, len(request.keywords), keyword[:30])

        try:
            category = await get_category_db_name(keyword=keyword + request.ref)

            content = await run_in_threadpool(
                gemini_new_gen,
                user_instructions=keyword,
                ref=request.ref,
                category=category,
            )

            if not content:
                log.error("ì›ê³  ìƒì„± ì‹¤íŒ¨", keyword=keyword[:20])
                continue

            image_urls = []
            if request.generate_images:
                images = await run_in_threadpool(
                    generate_images_parallel, keyword, request.image_count
                )
                image_urls = [img["url"] for img in images if img.get("url")]

            manuscript_id = await save_to_pending(keyword, content, image_urls)
            generated_ids.append(manuscript_id)
            log.success("ìƒì„± ì™„ë£Œ", id=manuscript_id, images=len(image_urls))

        except Exception as e:
            log.error("ìƒì„± ì—ëŸ¬", keyword=keyword[:20], error=str(e))

        await asyncio.sleep(1)

    if not generated_ids:
        raise HTTPException(status_code=500, detail="ì›ê³  ìƒì„±ì— ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    log.success("ì›ê³  ìƒì„± ì™„ë£Œ", count=len(generated_ids))

    # ========== 2ë‹¨ê³„: í ìƒì„± ==========
    log.header("2ë‹¨ê³„: í ìƒì„±", "ğŸ“¦")

    queue_id, queue_dir = create_queue(
        manuscript_ids=generated_ids,
        account_id=account_id,
        schedule_date=request.schedule_date,
    )

    manuscripts = get_queue_manuscripts(queue_id)
    log.kv("í ID", queue_id)
    log.kv("ì›ê³  ìˆ˜", len(manuscripts))

    # ========== 3ë‹¨ê³„: ë¡œê·¸ì¸ ==========
    log.header("3ë‹¨ê³„: ë„¤ì´ë²„ ë¡œê·¸ì¸", "ğŸ”")
    update_queue_status(queue_id, "processing")

    login_result = await naver_login_with_playwright(
        account_id=account_id,
        password=password,
        debug=True,
    )

    if not login_result["success"]:
        update_queue_status(queue_id, "failed")
        raise HTTPException(
            status_code=401, detail=f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {login_result.get('message')}"
        )

    cookies = login_result["cookies"]
    log.success("ë¡œê·¸ì¸ ì„±ê³µ", cookies=len(cookies))

    # ========== 4ë‹¨ê³„: ë°œí–‰ ==========
    log.header("4ë‹¨ê³„: ë¸”ë¡œê·¸ ë°œí–‰", "ğŸ“¤")

    base_time = get_base_time(request.schedule_date, request.schedule_start_hour)
    publish_results = []

    for idx, manuscript in enumerate(manuscripts):
        schedule_time = None

        if request.use_schedule:
            schedule_time = calculate_schedule_time(
                base_time, idx, request.schedule_interval_hours, 0
            )
            log.step(
                idx + 1,
                len(manuscripts),
                f"{manuscript.title[:25]} (ì˜ˆì•½: {schedule_time.strftime('%m/%d %H:%M')})",
            )
        else:
            log.step(idx + 1, len(manuscripts), f"{manuscript.title[:30]} (ì¦‰ì‹œ)")

        result = await publish_queue_manuscript(
            cookies=cookies,
            queue_dir=queue_dir,
            manuscript_id=manuscript.id,
            schedule_time=schedule_time,
            account_id=account_id,
        )
        publish_results.append(result)

        if idx < len(manuscripts) - 1:
            await asyncio.sleep(request.delay_between_posts)

    # ========== ê²°ê³¼ ==========
    success_count = sum(1 for r in publish_results if r["success"])
    cleanup_empty_queue(queue_id)

    elapsed = (datetime.now() - start_ts).total_seconds()

    log.divider()
    log.success(
        "ìë™í™” ì™„ë£Œ",
        queue_id=queue_id,
        ì„±ê³µ=f"{success_count}/{len(manuscripts)}",
        ì‹œê°„=f"{elapsed:.0f}s",
    )

    return JSONResponse(
        content={
            "success": True,
            "queue_id": queue_id,
            "account": f"{account_id[:3]}***",
            "generated": len(generated_ids),
            "published": success_count,
            "failed": len(manuscripts) - success_count,
            "elapsed": round(elapsed, 1),
            "results": publish_results,
        }
    )
