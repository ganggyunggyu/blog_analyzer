"""ë°°ì¹˜ ì›ê³  ìƒì„± API - í‚¤ì›Œë“œ ì—¬ëŸ¬ê°œ í•œë²ˆì— ì²˜ë¦¬ (ì›ê³  + ì´ë¯¸ì§€)"""

import asyncio
import time
import httpx
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from fastapi.concurrency import run_in_threadpool

from schema.generate import BatchGenerateRequest
from llm.gpt4o_service import gpt4o_gen
from llm.image_service import image_gen_single, get_random_poses
from utils.get_category_db_name import get_category_db_name
from utils.logger import log

router = APIRouter()

MANUSCRIPTS_DIR = Path("manuscripts")
PENDING_DIR = MANUSCRIPTS_DIR / "pending"
PENDING_DIR.mkdir(parents=True, exist_ok=True)


def get_next_manuscript_id() -> str:
    """ë‹¤ìŒ ì›ê³  ID ìƒì„±"""
    existing = list(PENDING_DIR.iterdir()) if PENDING_DIR.exists() else []
    max_id = 0
    for folder in existing:
        if folder.is_dir() and folder.name.isdigit():
            max_id = max(max_id, int(folder.name))
    return str(max_id + 1).zfill(4)


def generate_images_parallel(keyword: str, count: int) -> list[dict]:
    """ì´ë¯¸ì§€ ë³‘ë ¬ ìƒì„±"""
    poses = get_random_poses(count)
    images = []

    with ThreadPoolExecutor(max_workers=count) as executor:
        futures = {
            executor.submit(image_gen_single, keyword, pose): pose
            for pose in poses
        }

        for future in as_completed(futures):
            result = future.result()
            if result and result.get("url"):
                images.append(result)

    return images


async def download_image(url: str, save_path: Path) -> bool:
    """URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.get(url)
            if response.status_code == 200:
                with open(save_path, "wb") as f:
                    f.write(response.content)
                return True
    except Exception as e:
        log.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨", url=url[:50], error=str(e))
    return False


async def save_to_pending(keyword: str, content: str, image_urls: list[str] = None) -> str:
    """ìƒì„±ëœ ì›ê³ ì™€ ì´ë¯¸ì§€ë¥¼ pending í´ë”ì— ì €ì¥"""
    manuscript_id = get_next_manuscript_id()
    manuscript_dir = PENDING_DIR / manuscript_id
    manuscript_dir.mkdir(parents=True, exist_ok=True)

    # txt íŒŒì¼ë¡œ ì €ì¥
    safe_keyword = "".join(c for c in keyword[:20] if c.isalnum() or c in " _-").strip()
    txt_path = manuscript_dir / f"{safe_keyword or 'content'}.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(content)

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    if image_urls:
        download_tasks = []
        for idx, url in enumerate(image_urls):
            ext = Path(url).suffix or ".png"
            img_path = manuscript_dir / f"image_{idx + 1:02d}{ext}"
            download_tasks.append(download_image(url, img_path))

        results = await asyncio.gather(*download_tasks)
        downloaded = sum(1 for r in results if r)
        log.debug(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ", ì„±ê³µ=f"{downloaded}/{len(image_urls)}")

    return manuscript_id


@router.post("/generate/batch")
async def generate_batch(request: BatchGenerateRequest):
    """
    ë°°ì¹˜ ì›ê³  + ì´ë¯¸ì§€ ìƒì„±

    - keywords: í‚¤ì›Œë“œ ëª©ë¡
    - generate_images: ì´ë¯¸ì§€ ìƒì„± ì—¬ë¶€ (ê¸°ë³¸: True)
    - image_count: í‚¤ì›Œë“œë‹¹ ì´ë¯¸ì§€ ê°œìˆ˜ (ê¸°ë³¸: 5)
    """
    start_ts = time.time()
    service = request.service.lower()
    keywords = request.keywords
    ref = request.ref
    generate_images = request.generate_images
    image_count = request.image_count

    log.header(f"ë°°ì¹˜ ì›ê³  ìƒì„±", "ğŸ“¦")
    log.kv("ì„œë¹„ìŠ¤", service.upper())
    log.kv("í‚¤ì›Œë“œ ìˆ˜", len(keywords))
    log.kv("ì´ë¯¸ì§€ ìƒì„±", "ON" if generate_images else "OFF")

    results = []
    success_count = 0

    for idx, keyword in enumerate(keywords):
        keyword = keyword.strip()
        if not keyword:
            continue

        log.step(idx + 1, len(keywords), keyword[:30])

        try:
            # 1. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = await get_category_db_name(keyword=keyword + ref)

            # 2. ì›ê³  ìƒì„±
            content = await run_in_threadpool(
                gpt4o_gen,
                user_instructions=keyword,
                ref=ref,
                category=category
            )

            if not content:
                results.append({
                    "keyword": keyword,
                    "success": False,
                    "message": "ì›ê³  ìƒì„± ì‹¤íŒ¨",
                })
                log.error(f"ì›ê³  ìƒì„± ì‹¤íŒ¨", keyword=keyword[:20])
                continue

            # 3. ì´ë¯¸ì§€ ìƒì„± (ì˜µì…˜)
            image_urls = []
            if generate_images:
                log.debug(f"ì´ë¯¸ì§€ ìƒì„± ì¤‘...", count=image_count)
                images = await run_in_threadpool(
                    generate_images_parallel,
                    keyword,
                    image_count
                )
                image_urls = [img["url"] for img in images if img.get("url")]
                log.debug(f"ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ", count=len(image_urls))

            # 4. pending í´ë”ì— ì €ì¥
            manuscript_id = await save_to_pending(keyword, content, image_urls)
            success_count += 1

            results.append({
                "keyword": keyword,
                "success": True,
                "manuscript_id": manuscript_id,
                "content_length": len(content),
                "images_count": len(image_urls),
            })
            log.success(f"ì™„ë£Œ", keyword=keyword[:20], id=manuscript_id, images=len(image_urls))

        except Exception as e:
            results.append({
                "keyword": keyword,
                "success": False,
                "message": str(e),
            })
            log.error(f"ì—ëŸ¬", keyword=keyword[:20], error=str(e))

        # ìš”ì²­ ê°„ ë”œë ˆì´
        if idx < len(keywords) - 1:
            await asyncio.sleep(1)

    elapsed = time.time() - start_ts
    log.divider()
    log.success(f"ë°°ì¹˜ ì™„ë£Œ", ì„±ê³µ=f"{success_count}/{len(keywords)}", ì‹œê°„=f"{elapsed:.1f}s")

    return JSONResponse(content={
        "total": len(keywords),
        "success": success_count,
        "failed": len(keywords) - success_count,
        "elapsed": round(elapsed, 1),
        "results": results,
    })
