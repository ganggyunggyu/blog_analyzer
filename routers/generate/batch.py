"""ë°°ì¹˜ ì›ê³  ìƒì„± API - í‚¤ì›Œë“œ ì—¬ëŸ¬ê°œ í•œë²ˆì— ì²˜ë¦¬"""

import asyncio
import time
from datetime import datetime
from pathlib import Path
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from fastapi.concurrency import run_in_threadpool

from schema.generate import BatchGenerateRequest
from llm.gpt4o_service import gpt4o_gen
from utils.get_category_db_name import get_category_db_name
from utils.logger import log

router = APIRouter()

# pending í´ë” ê²½ë¡œ
MANUSCRIPTS_DIR = Path("manuscripts")
PENDING_DIR = MANUSCRIPTS_DIR / "pending"
PENDING_DIR.mkdir(parents=True, exist_ok=True)


def get_next_manuscript_id() -> str:
    """ë‹¤ìŒ ì›ê³  ID ìƒì„±"""
    existing = list(PENDING_DIR.iterdir()) if PENDING_DIR.exists() else []
    max_id = 0
    for folder in existing:
        if folder.is_dir() and folder.name.isdigit():
            max_id = max(max_id, int(folder.name))
    return str(max_id + 1).zfill(4)


def save_to_pending(keyword: str, content: str) -> str:
    """ìƒì„±ëœ ì›ê³ ë¥¼ pending í´ë”ì— ì €ì¥"""
    manuscript_id = get_next_manuscript_id()
    manuscript_dir = PENDING_DIR / manuscript_id
    manuscript_dir.mkdir(parents=True, exist_ok=True)

    # ì²« ì¤„ = ì œëª© (í‚¤ì›Œë“œ ê¸°ë°˜), ë‚˜ë¨¸ì§€ = ë³¸ë¬¸
    title = content.split('\n')[0].strip() if content else keyword

    # txt íŒŒì¼ë¡œ ì €ì¥
    txt_path = manuscript_dir / f"{keyword[:20]}.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(content)

    return manuscript_id


@router.post("/generate/batch")
async def generate_batch(request: BatchGenerateRequest):
    """
    ë°°ì¹˜ ì›ê³  ìƒì„± - í‚¤ì›Œë“œ ì—¬ëŸ¬ê°œ í•œë²ˆì— ì²˜ë¦¬
    ìƒì„±ëœ ì›ê³ ëŠ” ìë™ìœ¼ë¡œ pending í´ë”ì— ì €ì¥
    """
    start_ts = time.time()
    service = request.service.lower()
    keywords = request.keywords
    ref = request.ref

    log.header(f"ë°°ì¹˜ ì›ê³  ìƒì„± ì‹œì‘", "ğŸ“¦")
    log.kv("ì„œë¹„ìŠ¤", service.upper())
    log.kv("í‚¤ì›Œë“œ ìˆ˜", len(keywords))

    results = []
    success_count = 0

    for idx, keyword in enumerate(keywords):
        keyword = keyword.strip()
        if not keyword:
            continue

        log.step(idx + 1, len(keywords), keyword[:30])

        try:
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = await get_category_db_name(keyword=keyword + ref)

            # ì›ê³  ìƒì„±
            content = await run_in_threadpool(
                gpt4o_gen,
                user_instructions=keyword,
                ref=ref,
                category=category
            )

            if content:
                # pending í´ë”ì— ì €ì¥
                manuscript_id = save_to_pending(keyword, content)
                success_count += 1

                results.append({
                    "keyword": keyword,
                    "success": True,
                    "manuscript_id": manuscript_id,
                    "length": len(content),
                })
                log.success(f"ìƒì„± ì™„ë£Œ", keyword=keyword[:20], id=manuscript_id)
            else:
                results.append({
                    "keyword": keyword,
                    "success": False,
                    "message": "ìƒì„± ì‹¤íŒ¨",
                })
                log.error(f"ìƒì„± ì‹¤íŒ¨", keyword=keyword[:20])

        except Exception as e:
            results.append({
                "keyword": keyword,
                "success": False,
                "message": str(e),
            })
            log.error(f"ì—ëŸ¬", keyword=keyword[:20], error=str(e))

        # ìš”ì²­ ê°„ ë”œë ˆì´ (API ë¶€í•˜ ë°©ì§€)
        if idx < len(keywords) - 1:
            await asyncio.sleep(1)

    elapsed = time.time() - start_ts
    log.divider()
    log.success(f"ë°°ì¹˜ ì™„ë£Œ", ì„±ê³µ=f"{success_count}/{len(keywords)}", ì‹œê°„=f"{elapsed:.1f}s")

    return JSONResponse(content={
        "total": len(keywords),
        "success": success_count,
        "failed": len(keywords) - success_count,
        "elapsed": round(elapsed, 1),
        "results": results,
    })
